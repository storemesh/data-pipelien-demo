00 build docker-compose up -d

01 create env for workspace

02 connect lakefs and create repository

03 create postgres database with chinook dataset
  03.1 copy /sql_source/Chinook_PostgreSql.sql to postgres container using
    docker exec -it 04_data_pipeline_dev-postgresql_dev-1 psql -U admin -d mydb
    CREATE DATABASE chinook;
    docker cp sql_source/Chinook_PostgreSql.sql 04_data_pipeline_dev-postgresql_dev-1:/Chinook_PostgreSql.sql
    docker exec -it 04_data_pipeline_dev-postgresql_dev-1 psql -U admin -d chinook -f /Chinook_PostgreSql.sql
    
  03.2 copy /sql_source/northwind.sql to postgres container using (this step like 03.1 just change database name from chinook to northwind)
    docker exec -it 04_data_pipeline_dev-postgresql_dev-1 psql -U admin -d mydb
    CREATE DATABASE northwind;
    docker cp sql_source/northwind.sql 04_data_pipeline_dev-postgresql_dev-1:/northwind.sql
    docker exec -it 04_data_pipeline_dev-postgresql_dev-1 psql -U admin -d northwind -f /northwind.sql

04 if use pipeline that create just run this command
  04.1 cd to project "data-platform-00"
  04.2 python select_table.py
  04.3 kedro run pipeline=landing # the landing data is on lakefs
  04.4 kedro run pipeline=staging # show sample data from lakefs with dask dataframe

05 if use jupyter notebook 
  05.1 cd project "data-platform-00/notebooks"
  05.2 workplace sample is data_platform.ipynb
  05.3 from data_lib import landing_dsm # for use lib to landing data from postgres to lakefs 